{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f1a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from dataset import ExtractFeatures\n",
    "import pandas as pd\n",
    "import time\n",
    "import tracemalloc\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "import torchmetrics\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from PIL import UnidentifiedImageError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d580938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities = ('Philadelphia', 'New Orleans', 'Nashville', 'Tampa', 'Tucson', 'Indianapolis', 'Reno', 'Santa Barbara', 'Saint Louis', 'Boise')\n",
    "cities = ['Philadelphia']\n",
    "category = 'Restaurants'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f52aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"yelp_dataset/yelp_academic_dataset_review.csv\")\n",
    "businesses = pd.read_csv(\"yelp_dataset/yelp_academic_dataset_business.csv\",\n",
    "                         converters={'categories': lambda x: set([x.strip() for x in x.split(',')])})\n",
    "businesses.reset_index(inplace=True)\n",
    "photos = pd.read_csv(\"yelp_dataset/photos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32662f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# photos = photos[photos['label'] == 'inside']\n",
    "photos = photos[~(photos['label'] == 'menu')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# businesses.groupby('city')['review_count'].sum().reset_index().sort_values(by='review_count', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dfa870",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracemalloc.start()\n",
    "\n",
    "\n",
    "reviews['review_id'] = reviews['review_id'].astype(str)\n",
    "reviews['user_id'] = reviews['user_id'].astype(str)\n",
    "reviews['business_id'] = reviews['business_id'].astype(str)\n",
    "reviews['rating'] = reviews['rating'].astype(int)\n",
    "reviews['timestamp'] = pd.to_datetime(reviews['timestamp'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b7618",
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = businesses[businesses.city.isin(cities)]\n",
    "businesses = businesses[businesses['categories'].apply(lambda x: category in x)]\n",
    "business_map = {k: v for v, k in enumerate(businesses.index)}\n",
    "businesses['index'] = businesses['index'].map(business_map)\n",
    "businesses = businesses.drop(['state', 'review_count'], axis=1)\n",
    "businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "photos = photos.groupby(\"business_id\")[\"photo_id\"].apply(list).reset_index(name=\"photo_ids\")\n",
    "photos[\"photo_ids\"] = photos[\"photo_ids\"].apply(lambda x: x[0])\n",
    "photos = pd.merge(photos, businesses, left_on='business_id', right_on='id', how='right')\n",
    "photos = photos.drop(['id', 'name', 'city', 'categories', 'business_id'], axis=1)\n",
    "photos.columns = ['photo_ids', 'business_id']\n",
    "photos = photos.dropna()\n",
    "photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b68888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtractFeatures(model=\"resnet18\", cuda=True)\n",
    "\n",
    "failed = 0\n",
    "\n",
    "def img_to_vec(photo_id):\n",
    "    global failed\n",
    "    try:\n",
    "        if isinstance(photo_id, list):\n",
    "            return model.get_vec([Image.open(f'photos/{x}.jpg') for x in photo_id], tensor=True)\n",
    "        return model.get_vec(Image.open(f'photos/{photo_id}.jpg'))\n",
    "    except UnidentifiedImageError:\n",
    "        failed += 1\n",
    "        return None \n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "photos.photo_ids = photos.photo_ids.apply(img_to_vec)\n",
    "print(f'Time to produce {len(photos)} image embeddings: {(time.time() - start_time)//60} mins')\n",
    "photos_processed = len(photos)\n",
    "print(f'Total: {photos_processed}, failed: {failed}')\n",
    "photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d047c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "photos = photos.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f8318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.drop(['review_id'], axis = 1)\n",
    "reviews.columns = ['user_id', 'business_id', 'rating', 'timestamp']\n",
    "\n",
    "df = pd.merge(reviews, businesses, left_on=\"business_id\", right_on=\"id\")\n",
    "df = df.drop(['business_id', 'name', 'city', 'categories', 'id'], axis=1)\n",
    "df.columns = ['user_id', 'rating', 'timestamp', 'business_id']\n",
    "# df.to_csv('dataset_reset_index/reviews.csv', sep=\",\")\n",
    "df = pd.merge(df, photos, on=\"business_id\", how=\"left\")\n",
    "df = df.dropna()\n",
    "df['business_id'] = df['business_id'].astype(int)\n",
    "dct = {k: v for v, k in enumerate(df.business_id.unique())}\n",
    "df.business_id = df.business_id.map(dct)\n",
    "business_max = int(df.business_id.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a560a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_group = df.sort_values(by=[\"timestamp\"]).groupby(\"user_id\")\n",
    "\n",
    "\n",
    "reviews_data = pd.DataFrame(\n",
    "    data={\n",
    "        \"user_id\": list(reviews_group.groups.keys()),\n",
    "        \"business_ids\": list(reviews_group.business_id.apply(list)),\n",
    "        \"ratings\": list(reviews_group.rating.apply(list)),\n",
    "        \"timestamps\": list(reviews_group.timestamp.apply(list)),\n",
    "        \"photo_ids\": list(reviews_group.photo_ids.apply(list)),\n",
    "    }\n",
    ")\n",
    "\n",
    "reviews_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 8\n",
    "step_size = 2\n",
    "\n",
    "\n",
    "def create_sequences(values, window_size, step_size):\n",
    "    sequences = []\n",
    "    start_index = 0\n",
    "    while True:\n",
    "        end_index = start_index + window_size\n",
    "        seq = values[start_index:end_index]\n",
    "        if len(seq) < window_size:\n",
    "            seq = values[-window_size:]\n",
    "            if len(seq) == window_size:\n",
    "                sequences.append(seq)\n",
    "            break\n",
    "        sequences.append(seq)\n",
    "        start_index += step_size\n",
    "    return sequences\n",
    "\n",
    "\n",
    "reviews_data.business_ids = reviews_data.business_ids.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "reviews_data.ratings = reviews_data.ratings.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "reviews_data.photo_ids = reviews_data.photo_ids.apply(\n",
    "    lambda ids: create_sequences(ids, sequence_length, step_size)\n",
    ")\n",
    "\n",
    "del reviews_data[\"timestamps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61008d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data = reviews_data[reviews_data['ratings'].map(len) > step_size]\n",
    "\n",
    "dct = {k: v for v, k in enumerate(reviews_data.user_id.unique())}\n",
    "reviews_data.user_id = reviews_data.user_id.map(dct)\n",
    "\n",
    "user_max = int(reviews_data.user_id.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfddff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses.columns = ['business_id', 'business_yelp_id', 'name', 'city', 'categories']\n",
    "\n",
    "reviews_data_businesses = reviews_data[[\"user_id\", \"business_ids\"]].explode(\n",
    "    \"business_ids\", ignore_index=True\n",
    ")\n",
    "reviews_data_photos = reviews_data[[\"photo_ids\"]].explode(\"photo_ids\", ignore_index=True)\n",
    "reviews_data_rating = reviews_data[[\"ratings\"]].explode(\"ratings\", ignore_index=True)\n",
    "\n",
    "reviews_data_transformed = pd.concat([reviews_data_businesses, reviews_data_rating], axis=1)\n",
    "reviews_data_transformed = pd.concat([reviews_data_transformed, reviews_data_photos], axis=1)\n",
    "reviews_data_transformed = reviews_data_transformed.dropna()\n",
    "reviews_data_transformed.business_ids = reviews_data_transformed.business_ids.apply(\n",
    "    lambda x: \",\".join(str(i) for i in x)\n",
    ")\n",
    "\n",
    "reviews_data_transformed.columns = ['user_id', 'sequence_business_ids', 'sequence_ratings', 'sequence_photo_ids']\n",
    "reviews_data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea6f06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 0.8\n",
    "\n",
    "random_selection = np.random.rand(len(reviews_data_transformed.index)) <= train_percentage\n",
    "train_data = reviews_data_transformed[random_selection]\n",
    "test_data = reviews_data_transformed[~random_selection]\n",
    "\n",
    "\n",
    "seq_num = len(reviews_data_transformed)\n",
    "print(f'Sequences: {seq_num}')\n",
    "\n",
    "del df\n",
    "del reviews_data\n",
    "del photos\n",
    "del model\n",
    "del reviews_group\n",
    "del reviews_data_transformed\n",
    "del businesses\n",
    "del reviews\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "tracemalloc.stop()\n",
    "train_data.reset_index(inplace=True, drop=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf6163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpDataset(data.Dataset):\n",
    "    \"\"\"Yelp dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, ratings_file, test=False\n",
    "    ):\n",
    "        self.ratings_frame = ratings_file\n",
    "        self.test = test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.ratings_frame.iloc[idx]\n",
    "        user_id = data.user_id\n",
    "\n",
    "        business_history = eval(data.sequence_business_ids)\n",
    "        business_history_ratings = data.sequence_ratings\n",
    "#         print([x.shape for x in data.sequence_photo_ids])\n",
    "        photo_history = torch.stack(list(map(torch.from_numpy, data.sequence_photo_ids)))\n",
    "#         photo_history = torch.stack(data.sequence_photo_ids)\n",
    "        target_photo = photo_history[-1:][0]\n",
    "\n",
    "        target_business_id = business_history[-1:][0]\n",
    "        target_business_rating = business_history_ratings[-1:][0]\n",
    "\n",
    "        photo_history = photo_history[:-1]\n",
    "        business_history = torch.LongTensor(business_history[:-1])\n",
    "        business_history_ratings = torch.LongTensor(business_history_ratings[:-1])\n",
    "\n",
    "        return user_id, business_history, photo_history, target_business_id, business_history_ratings, target_business_rating, target_photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a134f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "class CustomTransformer(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self, learning_rate = 0.002, args=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.current_step = 0\n",
    "#         self.automatic_optimization = False\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.args = args\n",
    " \n",
    "        self.embeddings_user_id = nn.Embedding(user_max + 1, int(math.sqrt(user_max)) + 1)\n",
    "        self.embeddings_business_id = nn.Embedding(business_max + 1, 512)\n",
    "        self.embeddings_positions = nn.Embedding(sequence_length, 512)\n",
    "\n",
    "#         self.positions = self.positional_encoding(sequence_length - 1, 512)\n",
    "\n",
    "        # Network\n",
    "        self.transfomerlayer = nn.TransformerEncoderLayer(512, sequence_length, dropout=0.2, norm_first=False)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(4705, 1024,),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "        self.mae = torchmetrics.MeanAbsoluteError()\n",
    "        self.mse = torchmetrics.MeanSquaredError()\n",
    "\n",
    "    def encode_input(self, inputs):\n",
    "        user_id, business_history, photo_history, target_business_id, business_history_ratings, target_business_rating, target_photo = inputs\n",
    "\n",
    "        business_history = self.embeddings_business_id(business_history)\n",
    "        target_business = self.embeddings_business_id(target_business_id)\n",
    "        target_business = torch.unsqueeze(target_business, 1)\n",
    "        target_photo = torch.unsqueeze(target_photo, 1)\n",
    "\n",
    "        positions = torch.arange(0, sequence_length - 1, dtype=int, device=self.device)\n",
    "        positions = self.embeddings_positions(positions)\n",
    "\n",
    "\n",
    "        sequence = (positions + photo_history) * business_history_ratings[..., None]\n",
    "\n",
    "\n",
    "        transformer_features_x = torch.cat((sequence, target_business), 1)\n",
    "        transfomer_features = torch.cat((transformer_features_x, target_photo), 1)\n",
    "\n",
    "        user_id = self.embeddings_user_id(user_id)\n",
    "        user_features = user_id\n",
    "\n",
    "        return transfomer_features, user_features, target_business_rating.float()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        transfomer_features, user_features, target_business_rating = self.encode_input(batch)\n",
    "        transformer_output = self.transfomerlayer(transfomer_features)\n",
    "        transformer_output = torch.flatten(transformer_output, start_dim=1)\n",
    "\n",
    "        output = self.linear(torch.cat((transformer_output, user_features), dim=1))\n",
    "\n",
    "        return output, target_business_rating\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if self.opt is None:\n",
    "            self.opt = self.optimizers()\n",
    "            \n",
    "        self.current_step += 1\n",
    "            \n",
    "        out, target_business_rating = self(batch)\n",
    "        out = out.flatten()\n",
    "        loss = self.criterion(out, target_business_rating)\n",
    "\n",
    "        mae = self.mae(out, target_business_rating)\n",
    "        mse = self.mse(out, target_business_rating)\n",
    "        rmse = torch.sqrt(mse)\n",
    "        self.log(\n",
    "            \"train/mae\", mae, on_step=True, on_epoch=False, prog_bar=False\n",
    "        )\n",
    "\n",
    "        self.log(\n",
    "            \"train/rmse\", rmse, on_step=True, on_epoch=False, prog_bar=False\n",
    "        )\n",
    "\n",
    "        self.log(\"train/step_loss\", loss, on_step=True, on_epoch=False, prog_bar=False)\n",
    "        return loss\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        out, target_business_rating = self(batch)\n",
    "        out = out.flatten().tolist()\n",
    "#         target_business_rating = [int(x) for x in target_business_rating.flatten().tolist()]\n",
    "        recs = len(out)\n",
    "        \n",
    "        out = [0 if x < 3 else 1 for x in out]\n",
    "        target_business_rating = [0 if x < 3 else 1 for x in out]\n",
    "\n",
    "#         out = [int(x) if x < 3 else 5 for x in out]\n",
    "#         out = [0 if x < 3 else 1 for x in out]\n",
    "#         target_business_rating = [0 if x < 3 else 1 for x in out]\n",
    "\n",
    "        return out, target_business_rating\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        pred = []\n",
    "        target = []\n",
    "        for pair in (outputs):\n",
    "            pred.extend(pair[0])\n",
    "            target.extend(pair[1])\n",
    "        \n",
    "        score = f1_score(target, pred, average='macro', zero_division='warn')\n",
    "        score_weighted = f1_score(target, pred, average='weighted', zero_division='warn')\n",
    "        precision = precision_score(target, pred, average=\"macro\")\n",
    "        recall = recall_score(target, pred, average=\"macro\")\n",
    "            \n",
    "#         score = f1_score(target, pred, labels = [1,2,3,4,5], average='macro', zero_division='warn')\n",
    "#         score_weighted = f1_score(target, pred, labels = [1,2,3,4,5], average='weighted', zero_division='warn')\n",
    "#         precision = precision_score(target, pred, average=\"macro\")\n",
    "#         recall = recall_score(target, pred, average=\"macro\")\n",
    "        \n",
    "        self.log(\"test/precision\", precision, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"test/recall\", recall, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"test/f1\", score, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        self.log(\"test/f1_weighted\", score_weighted, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        \n",
    "        cm = confusion_matrix(target, pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        \n",
    "#         cm = confusion_matrix(target, pred, labels=[1,2,3,4,5])\n",
    "#         disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1,2,3,4,5])\n",
    "        disp.plot()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        out, target_business_rating = self(batch)\n",
    "        out = out.flatten()\n",
    "        loss = self.criterion(out, target_business_rating)\n",
    "\n",
    "        mae = self.mae(out, target_business_rating)\n",
    "        mse = self.mse(out, target_business_rating)\n",
    "        rmse = torch.sqrt(mse)\n",
    "\n",
    "        return {\"val_loss\": loss, \"mae\": mae.detach(), \"rmse\": rmse.detach()}\n",
    "        \n",
    "\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        avg_mae = torch.stack([x[\"mae\"] for x in outputs]).mean()\n",
    "        avg_rmse = torch.stack([x[\"rmse\"] for x in outputs]).mean()\n",
    "\n",
    "        self.log(\"val/loss\", avg_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/mae\", avg_mae, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val/rmse\", avg_rmse, on_step=False, on_epoch=True, prog_bar=False)\\\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), betas=(0.9, 0.98), lr=self.learning_rate)\n",
    "    \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, min_lr=1e-5, cooldown=2, patience=2)\n",
    "        self.opt = optimizer\n",
    "        return optimizer\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def positional_encoding(sequence_length, embedding_dim, cuda=True):\n",
    "        half_dim = embedding_dim // 2\n",
    "        emb = torch.log(torch.tensor(10000)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n",
    "        emb = torch.arange(sequence_length, dtype=torch.float).unsqueeze(\n",
    "            1\n",
    "        ) * emb.unsqueeze(0)\n",
    "        emb = torch.stack((torch.sin(emb), torch.cos(emb)), dim=0).view(\n",
    "            sequence_length, -1).t().contiguous().view(sequence_length, -1)\n",
    "        if embedding_dim % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros(sequence_length, 1)], dim=1)\n",
    "        if cuda:\n",
    "            return emb.cuda()\n",
    "        return emb\n",
    "    ####################\n",
    "    # DATA RELATED HOOKS\n",
    "    ####################\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        print(\"Loading datasets\")\n",
    "        self.train_dataset = YelpDataset(train_data)\n",
    "        self.val_dataset = YelpDataset(test_data)\n",
    "        self.test_dataset = YelpDataset(test_data)\n",
    "        print(\"Done\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=256,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c7a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e51f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1478836",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_gpu = pl.Trainer(accelerator='gpu', devices=1, max_epochs=1)\n",
    "model_gpu = CustomTransformer(learning_rate=0.0006)\n",
    "print(model_gpu)\n",
    "# lr_finder = trainer_gpu.tuner.lr_find(model_gpu,  min_lr=0.00005, max_lr=0.001, num_training=200, update_attr=False)\n",
    "# lr_fig = lr_finder.plot(suggest=True)\n",
    "# lr_fig.show()\n",
    "# print(f'Lighting suggested learning rate: {lr_finder.suggestion()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4679ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "trainer_gpu.fit(model_gpu)\n",
    "train_time = (time.time() - start_time)//60\n",
    "print(f'Training time: {train_time} mins')\n",
    "print(f'Batch size: {batch_size}')\n",
    "print(f'Images: {photos_processed}; Sequences: {seq_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f888e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_gpu.test(model_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bdaf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "data = [batch_size, photos_processed, seq_num]\n",
    "print(tabulate(data, headers=[\"Batch\",\"Photos\", \"Sequences\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98475553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataloader_tmp = torch.utils.data.DataLoader(\n",
    "            YelpDataset(test_data[np.random.rand(len(test_data.index)) <= .05]),\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "x = np.linspace(0, 20, 80)\n",
    "y = np.sin(x)\n",
    "plot_x, plot_y = [], []\n",
    "plt.clf()\n",
    "\n",
    "for batch_idx, batch in enumerate(model_gpu.test_dataloader()):\n",
    "    user_id, business_history, photo_history, target_business_id, business_history_ratings, target_business_rating, target_photo = batch\n",
    "    with torch.cuda.device(0):\n",
    "        out, target = model_gpu(batch)\n",
    "#     print(out)\n",
    "    out = out.detach().numpy()\n",
    "    plot_x.append(int(out[0]) if out[0] <= 5 else 5)\n",
    "    plot_y.append(target[0])\n",
    "plt.title(\"Predictions on a test dataset\")\n",
    "plt.plot(plot_y, color='blue', label='ratings')\n",
    "plt.plot(plot_x, color='red', alpha=0.7, label='predictions')\n",
    "plt.legend(['ratings', 'predictions'])\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd06a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visual-recommender",
   "language": "python",
   "name": "visual-recommender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
